<div class="page-content">
  <h2>Limitarile clusteringului cu modele de amestec si amestecuri de experti</h2>
  <p>Aceasta pagina discuta limitarile modelelor de amestec pentru clustering si introduce modelul de amestec de experti (Mixture of Experts - MoE) pentru regresie si clasificare.</p>
  <div class="definition-box"><strong>Limitarile amestecului de Bernoulli pentru MNIST:</strong> 1) Modelul pixel-independent nu captureaza structura spatiala. 2) K=10 poate fi insuficient (variatii de scriere). 3) Verosimilitatea non-convexa poate produce optime locale suboptimale. 4) Un bun estimator de densitate nu corespunde neaparat clusterelor dorite.</div>
  <div class="definition-box"><strong>Amestec de experti (MoE):</strong> p(y_i | x_i, z_i=k, &theta;) = N(y_i | w_k^T x_i, &sigma;_k&sup2;), p(z_i | x_i, &theta;) = Cat(z_i | S(V^T x_i)). Fiecare „expert" este un model de regresie liniara pe o regiune a inputului, iar functia de „portare" (gating) decide care expert este responsabil.</div>
  <div class="highlight-box"><strong>Predictia MoE:</strong> p(y_i | x_i, &theta;) = &Sigma;_k p(z_i=k | x_i, &theta;) p(y_i | x_i, z_i=k, &theta;). Media ponderata a predictiilor expertilor, cu ponderi date de functia de portare softmax. Aceasta produce modele de regresie neliniare din componente liniare.</div>
  <div class="commentary">
    <h3>Comentariu</h3>
    <div class="commentary-section"><h4>Rezumat</h4><p>Pagina face tranzitia de la modele de amestec pentru clustering (nesupervizat) la modele de amestec pentru regresie (supervizat). MoE imparte spatiul de intrare in regiuni, fiecare gestionata de un „expert" specializat. Diferenta cheie fata de modelele de amestec standard: ponderile de amestec depind de inputul x (prin functia de portare), nu sunt fixe. Aceasta permite specializarea adaptiva a modelului.</p></div>
    <div class="commentary-section"><h4>Tipare Cheie</h4><p>1) MoE = regresie liniara pe bucati cu tranzitii soft (ponderate de softmax). 2) Functia de portare invata automat partitionarea inputului. 3) Expertii pot fi orice model (nu doar regresie liniara) - retele neurale produc „mixture density networks". 4) MoE ierarhic: un expert este el insusi un MoE, producand o structura de arbore.</p></div>
    <div class="commentary-section"><h4>Aplicatii Practice</h4><p>MoE este folosit in sisteme de recomandare (diferiti „experti" pentru diferite tipuri de utilizatori), in robotica (modele diferite pentru diferite regimuri de miscare), in prognoza financiara (modele diferite pentru piete bull vs. bear), si mai recent in modele de limba mari (sparse MoE, precum Mixtral).</p></div>
    <div class="commentary-section"><h4>Exemplu din Lumea Reala</h4><p>Modelele de limba moderne precum Mixtral de la Mistral AI folosesc „Sparse Mixture of Experts": in loc de a activa toti neuronii pentru fiecare token, un router (functia de portare) selecteaza doar 2 din 8 experti. Aceasta reduce costul computational la 1/4 pastrand performanta totala, permitand modele mult mai mari la acelasi cost de inferenta.</p></div>
  </div>
</div>