<div class="page-content">
  <h2>Amestecuri de experti: vizualizare, functii de portare si DGM-uri</h2>
  <p>Aceasta pagina prezinta vizualizarea detaliata a modelului MoE (Figura 11.6) si structurile grafice asociate (Figura 11.7), inclusiv varianta ierarhica.</p>
  <div class="definition-box"><strong>Vizualizarea MoE (Figura 11.6):</strong> (a) Trei regresii liniare care se potrivesc pe regiuni diferite ale datelor. (b) Functiile de portare softmax: fiecare expert „domina" o regiune a inputului. (c) Predictia finala: media ponderata produce o curba smooth neliniara din trei componente liniare.</div>
  <div class="definition-box"><strong>MoE ierarhic (Figura 11.7b):</strong> Variabila latenta z are doua niveluri: z^1 selecteaza un sub-expert de nivel 1, care la randul sau are propria variabila z^2 pentru selectia sub-expertului de nivel 2. Aceasta produce o partitionare arborescenta a spatiului de intrare.</div>
  <div class="highlight-box"><strong>Flexibilitatea MoE:</strong> Orice model poate fi „plug in" ca expert: regresie liniara, retele neurale, procese Gaussiene, etc. Combinand retele neurale atat pentru portare cat si pentru experti, se obtin „mixture density networks" - modele capabile sa capteze distributii conditionate multimodale.</div>
  <div class="commentary">
    <h3>Comentariu</h3>
    <div class="commentary-section"><h4>Rezumat</h4><p>Figura 11.6 ofera o intelegere vizuala excelenta a MoE. In (a), cele trei linii de regresie se potrivesc diferit pe diferite regiuni. In (b), functiile de portare sigmoid/softmax arata tranzitii smooth intre experti. In (c), media ponderata produce o curba neliniara continua. DGM-ul din Figura 11.7 arata structura probabilistica: x_i genera z_i (prin portare), care impreuna cu x_i genera y_i.</p></div>
    <div class="commentary-section"><h4>Tipare Cheie</h4><p>1) Functiile de portare produc „frontiere de decizie" soft intre experti. 2) In regiunile de tranzitie, mai multi experti contribuie la predictie, asigurand continuitate. 3) MoE ierarhic produce partitionari mai fine cu mai putini parametri (structura de arbore vs. structura plata). 4) DGM-ul MoE difera de modelele de amestec standard prin sageata x_i &rarr; z_i (portarea depinde de input).</p></div>
    <div class="commentary-section"><h4>Aplicatii Practice</h4><p>Mixture density networks sunt folosite in sinteza vorbirii (distributia conditionata a sunetului este multimodala), in urmarirea obiectelor in video (mai multe pozitii posibile din cauza ocluziilor), si in robotica (probleme inverse cu solutii multiple).</p></div>
    <div class="commentary-section"><h4>Exemplu din Lumea Reala</h4><p>Figura 11.6 ar putea reprezenta relatia pret-cerere pentru un produs cu trei segmente de piata: clienti sensibili la pret (panta negativa mare), clienti fideli (panta mica), si clienti de lux (panta pozitiva - efect Veblen). Un singur model de regresie liniara ar fi inadecvat, dar MoE cu trei experti captureaza corect toate cele trei regimuri si produce o predictie nuantata.</p></div>
  </div>
</div>