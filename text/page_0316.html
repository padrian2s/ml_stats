<div class="page-content">
  <h2>Functia log-partitie ca generatoare de cumulanti</h2>
  <p>Aceasta pagina demonstreaza proprietatea remarcabila ca derivatele functiei log-partitie A(&theta;) genereaza cumulantele (media si varianta) distributiei.</p>
  <div class="definition-box"><strong>Prima derivata a log-partitiei:</strong> dA/d&theta; = E[&phi;(x)]. Prima derivata a functiei log-partitie in raport cu parametrul natural da media statisticii suficiente.</div>
  <div class="definition-box"><strong>A doua derivata a log-partitiei:</strong> d&sup2;A/d&theta;&sup2; = var[&phi;(x)]. A doua derivata da varianta statisticii suficiente. In cazul multivariat: &nabla;&sup2; A(&theta;) = cov[&phi;(x)].</div>
  <div class="highlight-box"><strong>Convexitatea lui A(&theta;):</strong> Deoarece covarianta este pozitiv-semidefinita, A(&theta;) este o functie convexa. Aceasta garanteaza ca log-verosimilitatea este concava, deci MLE are un unic maxim global - o proprietate fundamentala pentru optimizare.</div>
  <div class="commentary">
    <h3>Comentariu</h3>
    <div class="commentary-section"><h4>Rezumat</h4><p>Pagina demonstreaza unul dintre cele mai elegante rezultate ale statisticii: functia log-partitie â€žcodifica" toate momentele distributiei prin derivatele sale. Pentru Bernoulli: dA/d&theta; = sigm(&theta;) = &mu; (media), d&sup2;A/d&theta;&sup2; = &mu;(1-&mu;) (varianta). Convexitatea lui A(&theta;) asigura unicitatea MLE si convergenta algoritmilor de optimizare.</p></div>
    <div class="commentary-section"><h4>Tipare Cheie</h4><p>1) A(&theta;) codifica toata informatia despre distributie: derivata 1 = media, derivata 2 = varianta. 2) Acest lucru tine pentru orice membru al familiei exponentiale, nu doar Bernoulli sau Gaussiana. 3) Convexitatea lui A garanteaza unicitatea MLE (log-verosimilitatea este concava). 4) Demonstratia foloseste doar manipulari de integrale si derivate - nu necesita cunostinte specifice distributiei.</p></div>
    <div class="commentary-section"><h4>Aplicatii Practice</h4><p>Convexitatea log-verosimilitatii garanteaza ca algoritmii de gradient (SGD, Newton, IRLS) converg la solutia unica in toate modelele din familia exponentiala. Aceasta este o proprietate cruciala care nu este garantata in modelele de amestec sau in retelele neurale.</p></div>
    <div class="commentary-section"><h4>Exemplu din Lumea Reala</h4><p>In regresia Poisson (folosita in modelarea numarului de accidente), A(&theta;) = e^&theta;, dA/d&theta; = e^&theta; = &mu; (media), d&sup2;A/d&theta;&sup2; = e^&theta; = &mu; (varianta = media, proprietatea Poisson). Convexitatea garanteaza ca optimizarea va gasi solutia unica, indiferent de punctul de start - o garantie importanta cand modelul este folosit in politici de siguranta rutiera.</p></div>
  </div>
</div>