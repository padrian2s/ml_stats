<div class="page-content">
  <h2>Posteriorul predictiv si inferenta Bayesiana cu &sigma;&sup2; necunoscut</h2>
  <p>Aceasta pagina deriveaza distributia predictiva posterioara pentru regresia liniara si introduce inferenta cand atat coeficientii cat si varianta zgomotului sunt necunoscute.</p>
  <div class="definition-box"><strong>Posteriorul predictiv (cu &sigma;&sup2; cunoscut):</strong> p(y | x, D, &sigma;&sup2;) = N(y | w_N^T x, &sigma;_N&sup2;(x)), unde &sigma;_N&sup2;(x) = &sigma;&sup2; + x^T V_N x. Varianta predictiva are doua componente: zgomotul observatiei si incertitudinea parametrica.</div>
  <div class="definition-box"><strong>Priorul conjugat NIG:</strong> p(w, &sigma;&sup2;) = NIG(w, &sigma;&sup2; | w_0, V_0, a_0, b_0) = N(w | w_0, &sigma;&sup2; V_0) IG(&sigma;&sup2; | a_0, b_0). Covarianta priorului lui w este proportionala cu &sigma;&sup2; - o presupunere care asigura conjugarea.</div>
  <div class="highlight-box"><strong>Benzile de eroare variabile:</strong> Spre deosebire de aproximarea plug-in (care produce benzi de eroare constante), posteriorul predictiv Bayesian produce benzi de eroare care cresc departe de datele de antrenament. Aceasta este esentiala pentru invatarea activa, unde dorim sa stim nu doar ce credem, ci si cat de siguri suntem.</div>
  <div class="commentary">
    <h3>Comentariu</h3>
    <div class="commentary-section"><h4>Rezumat</h4><p>Pagina face distinctia cruciala intre aproximarea plug-in si predictia Bayesiana completa. Termenul x^T V_N x in varianta predictiva face ca incertitudinea sa depinda de locatia punctului de test: aproape de datele de antrenament varianta este mica, departe de ele este mare. Apoi se introduce priorul NIG pentru cazul mai realist cand si &sigma;&sup2; este necunoscut.</p></div>
    <div class="commentary-section"><h4>Tipare Cheie</h4><p>1) Varianta predictiva = varianta aleartica (&sigma;&sup2;, ireductibila) + varianta epistemica (x^T V_N x, reductibila prin date suplimentare). 2) Priorul NIG leaga incertitudinea coeficientilor de varianta zgomotului - o presupunere rezonabila. 3) Aproximarea plug-in subestimeaza incertitudinea, mai ales in regiunile cu putine date.</p></div>
    <div class="commentary-section"><h4>Aplicatii Practice</h4><p>Benzile de eroare variabile sunt esentiale in invatarea activa (alegerea urmatorului experiment acolo unde incertitudinea e maxima), in optimizarea Bayesiana (cautarea hiperparametrilor), si in sisteme critice de siguranta unde trebuie sa stim cand modelul â€žnu stie".</p></div>
    <div class="commentary-section"><h4>Exemplu din Lumea Reala</h4><p>Figura 7.12 din carte arata diferenta dramatic: (a) plug-in - benzi de eroare constante; (b) Bayesian - benzi care cresc la extreme. In practica, un model de prognoza a temperaturii bazat pe regresie liniara ar trebui sa arate incertitudine mai mare pentru predictii in conditii meteo neobisnuite (departe de datele de antrenament), nu aceeasi incertitudine ca pentru conditii tipice.</p></div>
  </div>
</div>