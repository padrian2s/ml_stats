<div class="page-content">
  <h2>Adagrad și SGD versus Învățarea Batch</h2>
  <p>Pagina introduce algoritmul Adagrad cu rate de învățare per parametru și discută avantajele SGD comparativ cu metodele batch, inclusiv pseudocodul complet al SGD.</p>
  <div class="highlight-box"><strong>Adagrad (Ecuația 8.84):</strong> θᵢ(k+1) = θᵢ(k) - η · gᵢ(k) / (τ₀ + √sᵢ(k)), unde sᵢ(k) = sᵢ(k-1) + gᵢ(k)² este suma gradienților la pătrat. Rata de învățare se adaptează per parametru la curbura funcției de pierdere.</div>
  <div class="highlight-box"><strong>Algoritmul 8.3 - SGD complet:</strong> 1) Inițializează θ, η; 2) Repetă: permută aleator datele; pentru i=1:N: g = ∇f(θ, zᵢ); θ ← proj_Θ(θ - ηg); actualizează η; până la convergență.</div>
  <div class="definition-box"><strong>Epoca (Epoch):</strong> O trecere completă prin întregul set de date de antrenament. SGD standard: B=1 (un exemplu), batch: B=N (toate), mini-batch: B~100 (compromis).</div>
  <div class="definition-box"><strong>Mini-batch SGD:</strong> Calculează gradientul pe un subset de B exemple la fiecare pas. Oferă un compromis între zgomotul SGD (B=1) și costul batch-ului complet (B=N). B~100 este tipic.</div>
  <div class="commentary">
    <h3>Comentariu</h3>
    <div class="commentary-section"><h4>Rezumat</h4><p>Adagrad rezolvă problema ratei de învățare globale prin adaptarea automată per parametru: parametrii cu gradienți mari primesc rate mici (deja au învățat mult), iar cei cu gradienți mici primesc rate mari (au nevoie de mai multă actualizare). SGD este preferat batch-ului pentru seturi de date mari deoarece: (1) estimarea gradientului din câteva exemple este adesea suficientă, (2) duplicarea datelor nu afectează SGD, (3) zgomotul SGD ajută la evitarea minimelor locale superficiale.</p></div>
    <div class="commentary-section"><h4>Tipare Cheie</h4><p>Adagrad introduce o aproximare diagonală a Hessianului inverse prin acumularea gradienților la pătrat. Aceasta conectează metodele de ordinul întâi cu cele de ordinul doi. Argumentul „duplicarea datelor" este deosebit de elegant: batch-ul pe 2N date identice durează de 2 ori mai mult, dar SGD nu este afectat deoarece direcția gradientului nu se schimbă.</p></div>
    <div class="commentary-section"><h4>Aplicatii Practice</h4><p>Adagrad a inspirat algoritmi mai avansați: RMSprop (cu medie exponențială în loc de sumă), Adam (combină momentum cu rate adaptive), AdaFactor (eficient în memorie). Mini-batch SGD cu GPU-uri permite paralelism: fiecare exemplu din mini-batch poate fi procesat independent.</p></div>
    <div class="commentary-section"><h4>Exemplu din Lumea Reala</h4><p>Un model de recomandare cu milioane de utilizatori și produse are parametri foarte diferiți: produsele populare au gradienți frecvenți, cele rare au gradienți sporadici. Adagrad oferă automat rate mai mari produselor rare, asigurând că și acestea sunt învățate adecvat.</p></div>
  </div>
</div>
